import re
from snowballstemmer import stemmer
from pathlib import Path

class TextProcessingUtils:
    stop_words = set()
    english_stemmer = stemmer("english")
    clean_regex = re.compile("[^a-zA-Z0-9]")
    split_regex = re.compile(" ")

    def load_stopwords(cls, stopwords_path):
        with open(stopwords_path, 'r', encoding='utf-8') as file:
            cls.stop_words = set(file.read().splitlines())

    def is_stopword(cls, token):
        return token in cls.stop_words

    def truncate_token(cls, token):
        return token[:Constants.MAX_TERM_LEN] if len(token) > Constants.MAX_TERM_LEN else token

    def tokenize(cls, document):

        document = document.lower()
        # Rimozione di punteggiatura e caratteri strani != [^a-zA-Z0-9]
        document = cls.clean_regex.sub(" ", document)
        # Suddivisione in token
        return cls.split_regex.split(document)

    def stem_token(cls, token):
        cls.english_stemmer.setCurrent(token)
        if cls.english_stemmer.stem():
            token = cls.english_stemmer.getCurrent()
        return token